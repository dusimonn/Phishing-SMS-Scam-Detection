{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1352062`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Infrastructure and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tokens(processed_text):\n",
    "    \"\"\"\n",
    "    Given a preprocessed string/instance, return its tokens and their counts.\n",
    "    \"\"\"\n",
    "    token_counts = dict()\n",
    "    \n",
    "    if isinstance(processed_text, str):\n",
    "        # Split the string into words/tokens\n",
    "        processed_text = processed_text.split(\" \")\n",
    "        # Count the occurrences of each token\n",
    "        for token in processed_text:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "    else:\n",
    "        # Empty string classified as a float for some reason\n",
    "        token_counts = {\"\": 1}\n",
    "\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Find the list of every word which occurs in the  dataset (every word in textPreprocessed)\n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        processed_text = return_tokens(data.loc[i, \"textPreprocessed\"]).keys()\n",
    "        vocabulary = vocabulary + list(processed_text)\n",
    "\n",
    "    vocabulary = sorted(list(set(vocabulary)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_count_matrix(data, vocabulary):\n",
    "    \"\"\"\n",
    "    Find the count matrix, which is a matrix of size N*V where N is the number of instances in the training data \n",
    "    and V is the number of words in the vocabulary.  \n",
    "    Each cell in the matrix represents the number of times a given word appeared in a given message \n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    # Intialise matrix N*V with 0s\n",
    "    count_matrix = np.zeros((N,V))\n",
    "    count_matrix = pd.DataFrame(count_matrix)\n",
    "    count_matrix.columns = vocabulary\n",
    "\n",
    "    # Iterate over each row - instance \n",
    "    for i in range(N):\n",
    "        instance_token_counts = return_tokens(data.loc[i, \"textPreprocessed\"])\n",
    "        for token in instance_token_counts.keys():\n",
    "            count_matrix.loc[i, token] = instance_token_counts[token]\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prior_prob(data):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of the prior probability of each class P(class)\n",
    "    \"\"\"\n",
    "    prior_probs = dict()\n",
    "\n",
    "    labels = np.unique(data[\"class\"])\n",
    "    for label in labels:\n",
    "        prior_probs[int(label)] = len(data[data[\"class\"]==label])/len(data)\n",
    "\n",
    "    return prior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_c_i(data, count_matrix, label, word, V=0, alpha=0):\n",
    "    \"\"\"\n",
    "    p(word | label) = count(c,i) + alpha / total(c) + V*alpha\n",
    "    count(c,i) is the total count of times word i appears in messages from class c\n",
    "    total(c) is the total count of words in class c\n",
    "    Alpha is smoothing factor used for Laplace smoothing, defaulted to 0 (no smoothing)\n",
    "    V is the length of the count vector of a test instance\n",
    "    \"\"\"\n",
    "    label_indexes = data[data[\"class\"] == label].index\n",
    "    count_matrix = count_matrix.iloc[label_indexes]\n",
    "    word_counts = np.sum(count_matrix[word]) + alpha\n",
    "    label_counts = np.sum(count_matrix, axis=0).sum() + (V*alpha)\n",
    "    \n",
    "    return float(word_counts / label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_conditional_prob(data, vocabulary, count_matrix):\n",
    "    \"\"\"\n",
    "    Return dictionary conditional probabilities of each token in a class\n",
    "    \"\"\"\n",
    "    conditional_probs = {int(label): None for label in data[\"class\"].unique()}\n",
    "    for label in conditional_probs.keys():\n",
    "        conditional_probs[label] = {word: 0 for word in vocabulary}\n",
    "\n",
    "        for word in vocabulary:\n",
    "            conditional_probs[label][word] = calc_p_c_i(data, count_matrix, label, word, V=len(vocabulary))\n",
    "    \n",
    "    return conditional_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_test_count_vector(test_text, vocabulary):\n",
    "    \"\"\"\n",
    "    Find the count vector of a test instance\n",
    "    \"\"\"\n",
    "    count_vector = np.zeros((1, len(vocabulary)))[0]\n",
    "    test_token_counts = return_tokens(test_text)\n",
    "\n",
    "    for i in range(len(vocabulary)):\n",
    "        token = vocabulary[i]\n",
    "        if token in test_token_counts.keys():\n",
    "            count_vector[i] = test_token_counts[token]\n",
    "    \n",
    "    return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_posterior_prob(data, count_vector, vocabulary, prior_probs, conditional_probs):\n",
    "    \"\"\"\n",
    "    Find posterior probability P(class = label | count = count vector) -> P(class = label) * P(count|c)\n",
    "    \"\"\"\n",
    "    posterior_probs = {int(label): None for label in data[\"class\"].unique()}\n",
    "    for label in posterior_probs.keys():\n",
    "        p_class = prior_probs[label]\n",
    "        p_count_c = math.factorial(int(sum(count_vector))) / math.prod([math.factorial(int(x)) for x in count_vector])\n",
    "        test_token_indexes = [i for i in range(len(count_vector)) if count_vector[i] != 0]\n",
    "\n",
    "        for token_index in test_token_indexes:\n",
    "            p_c_i = conditional_probs[label][vocabulary[token_index]]\n",
    "            p_count_c = p_count_c * math.pow(p_c_i, count_vector[token_index])\n",
    "\n",
    "        p_c_count = p_class * p_count_c\n",
    "        posterior_probs[label] = p_c_count\n",
    "\n",
    "    return posterior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_instance(data, vocabulary, test_text, prior_probs, conditional_probs):\n",
    "    \"\"\"\n",
    "    Given a string, classify it as 0 - non malicious, or 1 - scam\n",
    "    \"\"\"\n",
    "    test_count_vector = find_test_count_vector(test_text, vocabulary)\n",
    "\n",
    "    # Count vector all zeroes so cannot classify\n",
    "    if np.count_nonzero(test_count_vector) == 0:\n",
    "        return None\n",
    "    \n",
    "    test_posterior_prob = find_posterior_prob(data, test_count_vector, vocabulary, prior_probs, conditional_probs)\n",
    "\n",
    "    # Find which class has higher likelihood\n",
    "    best_label = None\n",
    "    best_prob = 0\n",
    "\n",
    "    # If both are equal, return 0.5, hard coded \n",
    "    if test_posterior_prob[0] == test_posterior_prob[1]:\n",
    "        return 0.5\n",
    "\n",
    "    for label, prob_label in test_posterior_prob.items():\n",
    "        if prob_label > best_prob:\n",
    "            best_prob = prob_label\n",
    "            best_label = label\n",
    "    \n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"sms_supervised_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = find_vocabulary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = find_count_matrix(train, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_probs = find_prior_prob(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probs = find_conditional_prob(train, vocabulary, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1.0000000000000027)\n",
      "(1, 0.9999999999999903)\n"
     ]
    }
   ],
   "source": [
    "# check if prob within each class label sum = 1\n",
    "for label in conditional_probs.keys():\n",
    "    print((label, sum(conditional_probs[label].values())))\n",
    "# good enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"sms_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = test[\"textPreprocessed\"]\n",
    "predicted_labels = []\n",
    "for test_instance in test_instances:\n",
    "    predicted_label = classify_test_instance(test, vocabulary, test_instance, prior_probs, conditional_probs)\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "test[\"predicted class\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>textPreprocessed</th>\n",
       "      <th>class</th>\n",
       "      <th>predicted class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's up? Do you want me to come online? If y...</td>\n",
       "      <td>? ? up come online free talk sometime �</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't quite know what to do. I still can't g...</td>\n",
       "      <td>? . . up know quite still get hold anyone cud ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>House-Maid is the murderer, coz the man was mu...</td>\n",
       "      <td>.. , house-maid murderer coz man murder 26th j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ok i thk i got it. Then u wan me 2 come now or...</td>\n",
       "      <td>? . 2 come u get ok thk wan wat</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello from Orange. For 1 month's free access t...</td>\n",
       "      <td>1 . . reply free free , , , message 's yes tex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Well i will watch shrek in 3D!!B)</td>\n",
       "      <td>! ! ) well b watch 3d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Babe, I'm back ... Come back to me ...</td>\n",
       "      <td>come , back back ... ... babe</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Got it! It looks scrumptious... daddy wants to...</td>\n",
       "      <td>get look ... ! ! eat night want long daddy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Especially since i talk about boston all up in...</td>\n",
       "      <td>. up talk , ! ! change personal lol especially...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>:( but your not here....</td>\n",
       "      <td>:( ....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          textOriginal  \\\n",
       "0    What's up? Do you want me to come online? If y...   \n",
       "1    I don't quite know what to do. I still can't g...   \n",
       "2    House-Maid is the murderer, coz the man was mu...   \n",
       "3    Ok i thk i got it. Then u wan me 2 come now or...   \n",
       "4    Hello from Orange. For 1 month's free access t...   \n",
       "..                                                 ...   \n",
       "995                  Well i will watch shrek in 3D!!B)   \n",
       "996             Babe, I'm back ... Come back to me ...   \n",
       "997  Got it! It looks scrumptious... daddy wants to...   \n",
       "998  Especially since i talk about boston all up in...   \n",
       "999                           :( but your not here....   \n",
       "\n",
       "                                      textPreprocessed  class  predicted class  \n",
       "0              ? ? up come online free talk sometime �      0              0.0  \n",
       "1    ? . . up know quite still get hold anyone cud ...      0              0.0  \n",
       "2    .. , house-maid murderer coz man murder 26th j...      0              0.0  \n",
       "3                      ? . 2 come u get ok thk wan wat      0              0.0  \n",
       "4    1 . . reply free free , , , message 's yes tex...      1              1.0  \n",
       "..                                                 ...    ...              ...  \n",
       "995                              ! ! ) well b watch 3d      0              0.0  \n",
       "996                      come , back back ... ... babe      0              0.0  \n",
       "997         get look ... ! ! eat night want long daddy      0              0.0  \n",
       "998  . up talk , ! ! change personal lol especially...      0              0.5  \n",
       "999                                            :( ....      0              0.0  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    text = test.loc[i, \"textPreprocessed\"]\n",
    "    text_tokens = return_tokens(text)\n",
    "    all_tokens_not_in_vocab = True\n",
    "\n",
    "    for token in text_tokens:\n",
    "        if token in vocabulary:\n",
    "            all_tokens_not_in_vocab = False\n",
    "            break\n",
    "\n",
    "    if all_tokens_not_in_vocab:\n",
    "        print(test.loc[i, \"textPreprocessed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled = pd.read_csv(\"sms_unlabelled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
